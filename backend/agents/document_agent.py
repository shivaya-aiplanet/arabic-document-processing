#!/usr/bin/env python3
"""
Document Processing Agent using Autogen + Groq Llama
Specialized for Arabic government document analysis
"""
import json
import time
import asyncio
from typing import Dict, Any, Optional
from PIL import Image
import requests

try:
    import autogen
    from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager
except ImportError:
    # Fallback if autogen is not available
    autogen = None
    AssistantAgent = None
    UserProxyAgent = None
    GroupChat = None
    GroupChatManager = None

class DocumentProcessingAgent:
    """Agent-based document processing using Autogen + Groq Llama"""
    
    def __init__(self, groq_api_key: str, qari_client=None):
        self.groq_api_key = groq_api_key
        self.qari_client = qari_client
        
        # Configure Groq LLM for Autogen
        self.llm_config = {
            "config_list": [
                {
                    "model": "llama3-70b-8192",
                    "api_key": groq_api_key,
                    "base_url": "https://api.groq.com/openai/v1",
                    "api_type": "openai"
                }
            ],
            "temperature": 0.1,
            "max_tokens": 2000,
            "timeout": 60
        }
        
        # Initialize agents
        self._setup_agents()
    
    def _setup_agents(self):
        """Setup Autogen agents for document processing"""
        
        # OCR Specialist Agent
        self.ocr_agent = AssistantAgent(
            name="OCR_Specialist",
            system_message="""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ©. 
            Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ù† OCR ÙˆØªÙ†Ø¸ÙŠÙÙ‡ ÙˆØªØ­Ø³ÙŠÙ†Ù‡.
            
            Ù‚Ù… Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:
            1. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ OCR
            2. ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ© Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
            3. ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø·Ù‚ÙŠ
            4. Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø£ØµÙ„ÙŠ
            
            Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·.""",
            llm_config=self.llm_config
        )
        
        # Entity Extraction Agent
        self.extraction_agent = AssistantAgent(
            name="Entity_Extractor",
            system_message="""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø®Ø· Ø§Ù„ÙŠØ¯.

            Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
            1. Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ù‚ÙŠÙ…Ø©ØŒ ØªØ¬Ù†Ø¨ ÙƒØªØ§Ø¨Ø© Ø§Ù„ØªØ³Ù…ÙŠØ© (Ù…Ø«Ù„ "Ø§Ù„Ø§Ø³Ù…") ÙƒÙ‚ÙŠÙ…Ø© Ù„Ù„Ø­Ù‚Ù„
            2. Ø§Ø¨Ø­Ø« Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¹Ù† Ø§Ù„Ù†Øµ Ø§Ù„ÙØ±ÙŠØ¯ Ø£Ùˆ Ø§Ù„Ù…ÙƒØªÙˆØ¨ Ø¨Ø®Ø· Ø§Ù„ÙŠØ¯ Ø¨Ø¬ÙˆØ§Ø± Ø£Ùˆ ØªØ­Øª Ø§Ù„ØªØ³Ù…ÙŠØ©ØŒ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù† ØºÙŠØ± ÙˆØ§Ø¶Ø­
            3. Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ù†Øµ ÙˆØ§Ø¶Ø­Ø§Ù‹ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ØŒ Ø§Ø³ØªØ®Ø±Ø¬ Ù…Ø§ ØªØ³ØªØ·ÙŠØ¹ ÙˆØ§Ù…Ù„Ø£ Ø§Ù„Ø¨Ø§Ù‚ÙŠ Ø¨Ù€ "(ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©)"
            4. Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø£ÙƒØ«Ø± Ù…Ù† Ù‚ÙŠÙ…Ø© Ù…Ø­ØªÙ…Ù„Ø©ØŒ Ø§Ø®ØªØ± Ø§Ù„Ø£ÙƒØ«Ø± ØªÙ…ÙŠØ²Ø§Ù‹ Ø£Ùˆ Ø§Ù„ØªÙŠ ØªØ¨Ø¯Ùˆ Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø®Ø· Ø§Ù„ÙŠØ¯
            5. Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª ØªØ§Ø±ÙŠØ®Ø§Ù‹ Ø£Ùˆ Ø±Ù‚Ù… Ù‡ÙˆÙŠØ© Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù† Ø¬Ø²Ø¦ÙŠØ§Ù‹ØŒ Ø§Ø³ØªØ®Ø±Ø¬Ù‡ ÙƒÙ…Ø§ Ù‡Ùˆ
            6. Ù„Ø§ ØªÙƒØ±Ø± Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ø£ÙƒØ«Ø± Ù…Ù† Ø­Ù‚Ù„ØŒ ÙˆÙ„Ø§ ØªØªØ±Ùƒ Ø£ÙŠ Ø­Ù‚Ù„ ÙØ§Ø±ØºØ§Ù‹ Ø¥Ù„Ø§ Ø¥Ø°Ø§ Ø§Ø³ØªØ­Ø§Ù„ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬
            7. Ù„Ø§ ØªÙƒØ±Ø± Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØªÙŠ ØªØ¸Ù‡Ø± ÙƒÙ€ "Ø¥ÙØ§Ø¯Ø© Ø³ÙƒÙ†" Ø£Ùˆ "ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©" ÙÙŠ ØºÙŠØ± Ù…Ø­Ù„Ù‡Ø§

            Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬:
            - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ÙØ±ÙŠØ¯Ø© ÙˆØ§Ù„Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø®Ø· Ø§Ù„ÙŠØ¯
            - ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…Ø·Ø¨ÙˆØ¹Ø© ÙˆØ§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©
            - Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ù†Ø§Ù‚ØµØ© (Ù…Ø«Ù„ "Ø£Ø­Ù…Ø¯ Ø³" Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† "ØºÙŠØ± Ù…ØªÙˆÙØ±")
            - Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø¬Ø²Ø¦ÙŠØ©
            - Ø§Ø±Ø¨Ø· ÙƒÙ„ Ø­Ù‚Ù„ Ø¨Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙƒØ«Ø± Ù…Ù†Ø·Ù‚ÙŠØ© ÙˆØªÙ…ÙŠØ²Ø§Ù‹

            ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¬ÙˆØ§Ø¨ JSON ØµÙØ±Ù ÙƒÙ…Ø§ Ù‡Ùˆ ÙÙŠ Ø§Ù„Ù…Ø«Ø§Ù„:
            {
                "Ø±Ù‚Ù…_Ø§Ù„Ù…Ø³ØªÙ†Ø¯": "117-11-2018",
                "Ø§Ù„ØªØ§Ø±ÙŠØ®": "11/11/2018",
                "Ù†ÙˆØ¹_Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©": "Ø¥ÙØ§Ø¯Ø© Ø³ÙƒÙ†",
                "Ø§Ù„Ø¬Ù‡Ø©_Ø§Ù„ØµØ§Ø¯Ø±Ø©": "ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© ÙˆØ§Ù„Ø¨Ù„Ø¯ÙŠØ§Øª",
                "Ø§Ù„Ø§Ø³Ù…_Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ": "Ù…Ø­Ù…Ø¯ Ø³Ù„ÙŠÙ… (ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©)",
                "Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„": "Ø§Ø³Ù… Ù…Ø³Ø¤ÙˆÙ„ (ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©)",
                "Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹": "Ø¥ÙØ§Ø¯Ø© Ø³ÙƒÙ†"
            }

            Ø£Ø¬Ø¨ Ø¨Ù€ JSON ØµØ­ÙŠØ­ ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ.""",
            llm_config=self.llm_config
        )
        
        # Document Classifier Agent
        self.classifier_agent = AssistantAgent(
            name="Document_Classifier",
            system_message="""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ©.
            
            ØµÙ†Ù Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø¥Ù„Ù‰ Ø¥Ø­Ø¯Ù‰ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:
            - Ø´Ù‡Ø§Ø¯Ø©_Ù…Ù„ÙƒÙŠØ©: Ø´Ù‡Ø§Ø¯Ø§Øª Ù…Ù„ÙƒÙŠØ© Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø£Ø±Ø§Ø¶ÙŠ
            - Ø®Ø·Ø§Ø¨_ØªØ­ÙˆÙŠÙ„: Ø®Ø·Ø§Ø¨Ø§Øª Ù†Ù‚Ù„ Ø£Ùˆ ØªØ­ÙˆÙŠÙ„
            - Ù†Ù…ÙˆØ°Ø¬_Ø§Ø¹Ø±Ù_Ø¹Ù…ÙŠÙ„Ùƒ: Ù†Ù…Ø§Ø°Ø¬ KYC ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‡ÙˆÙŠØ©
            - ØªÙ‚Ø±ÙŠØ±_Ù…Ø±Ø§Ø¬Ø¹Ø©: ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© ÙˆØ§Ù„ØªØ¯Ù‚ÙŠÙ‚
            - Ø¥ÙŠØµØ§Ù„_Ø§Ø³ØªÙ„Ø§Ù…: Ø¥ÙŠØµØ§Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªÙ„Ø§Ù… ÙˆØ§Ù„ØªØ³Ù„ÙŠÙ…
            - ÙˆØ«ÙŠÙ‚Ø©_Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©: Ø§Ù„Ø¹Ù‚ÙˆØ¯ ÙˆØ§Ù„Ø§ØªÙØ§Ù‚ÙŠØ§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
            - Ù…Ø¹Ø§Ù…Ù„Ø©_Ù…Ø§Ù„ÙŠØ©: Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…ØµØ±ÙÙŠØ© ÙˆØ§Ù„Ù…Ø§Ù„ÙŠØ©
            - Ø®Ø¯Ù…Ø©_Ø­ÙƒÙˆÙ…ÙŠØ©: Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ©
            - Ø£Ø®Ø±Ù‰: Ø£ÙŠ Ù†ÙˆØ¹ Ø¢Ø®Ø±
            
            Ù‚Ø¯Ù… Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON:
            {
                "Ù†ÙˆØ¹_Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©": "Ø§Ù„ØªØµÙ†ÙŠÙ",
                "Ù…Ø³ØªÙˆÙ‰_Ø§Ù„Ø«Ù‚Ø©": "Ø¹Ø§Ù„ÙŠ/Ù…ØªÙˆØ³Ø·/Ù…Ù†Ø®ÙØ¶",
                "Ø§Ù„Ø³Ø¨Ø¨": "Ø³Ø¨Ø¨ Ø§Ù„ØªØµÙ†ÙŠÙ",
                "Ø®ØµØ§Ø¦Øµ_Ù…Ù…ÙŠØ²Ø©": ["Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ù…ÙŠØ²Ø©"]
            }""",
            llm_config=self.llm_config
        )
        
        # Data Review Agent
        self.review_agent = AssistantAgent(
            name="Data_Reviewer",
            system_message="""Ø£Ù†Øª Ù…Ø±Ø§Ø¬Ø¹ Ø®Ø¨ÙŠØ± Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø®Ø· Ø§Ù„ÙŠØ¯.

            Ù…Ù‡Ù…ØªÙƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
            1. Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© ÙˆØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯ØªÙ‡Ø§ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙƒØªÙˆØ¨ Ø¨Ø®Ø· Ø§Ù„ÙŠØ¯
            2. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…Ø·Ø¨ÙˆØ¹Ø© ÙˆØ§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ù…ØªÙƒØ±Ø±Ø© Ù…Ù† Ø§Ù„Ù‚ÙŠÙ…
            3. Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙØ±ÙŠØ¯ ÙˆØ§Ù„Ù…ÙƒØªÙˆØ¨ Ø¨Ø®Ø· Ø§Ù„ÙŠØ¯ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù† Ù†Ø§Ù‚ØµØ§Ù‹
            4. Ø¥Ø¶Ø§ÙØ© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ù„Ù„Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ø§Ù„ÙˆØ§Ø¶Ø­
            5. ØªØ¬Ù†Ø¨ ØªØ±Ùƒ Ø§Ù„Ø­Ù‚ÙˆÙ„ ÙØ§Ø±ØºØ© Ø¥Ù„Ø§ Ø¥Ø°Ø§ Ø§Ø³ØªØ­Ø§Ù„ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬

            Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©:
            - Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„ØªÙˆØ§Ø±ÙŠØ® ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø¬Ø²Ø¦ÙŠØ©
            - Ø£Ø²Ù„ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ù…Ø«Ù„ "Ø§Ù„Ø§Ø³Ù…:" Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… ÙˆØ§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙÙ‚Ø·
            - Ø£Ø¶Ù "(ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©)" Ù„Ù„Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ø§Ù„ÙˆØ§Ø¶Ø­
            - ØªØ¬Ù†Ø¨ ØªÙƒØ±Ø§Ø± Ù†ÙØ³ Ø§Ù„Ù‚ÙŠÙ…Ø© ÙÙŠ Ø­Ù‚ÙˆÙ„ Ù…Ø®ØªÙ„ÙØ©
            - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙØ±ÙŠØ¯ ÙˆØ§Ù„Ù…Ù…ÙŠØ²

            Ø£Ø¬Ø¨ Ø¨Ù€ JSON ØµØ­ÙŠØ­ ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ.""",
            llm_config=self.llm_config
        )
        
        # User Proxy Agent (orchestrator)
        self.user_proxy = UserProxyAgent(
            name="Document_Processor",
            human_input_mode="NEVER",
            max_consecutive_auto_reply=1,
            code_execution_config=False
        )
    
    async def process_document_page(self, image: Image.Image, page_number: int, filename: str) -> Dict[str, Any]:
        """Process a single document page through the complete pipeline"""
        
        start_time = time.time()
        result = {
            "page_number": page_number,
            "filename": filename,
            "success": False,
            "processing_time": 0,
            "ocr_result": {},
            "agent_result": {},
            "error": None
        }
        
        try:
            # Step 1: OCR with QARI
            print(f"ðŸ”„ Page {page_number}: Running OCR...")
            if self.qari_client:
                ocr_result = await self.qari_client.extract_text(image)
                result["ocr_result"] = ocr_result
                
                if not ocr_result.get("success"):
                    result["error"] = f"OCR failed: {ocr_result.get('error')}"
                    return result
                
                extracted_text = ocr_result.get("text", "")
            else:
                # Fallback: simulate OCR for testing
                extracted_text = "Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± - ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© Ù‡Ù†Ø§"
                result["ocr_result"] = {
                    "success": True,
                    "text": extracted_text,
                    "processing_time": 1.0
                }
            
            print(f"âœ… Page {page_number}: OCR completed ({len(extracted_text)} chars)")
            
            # Step 2: Process with agents
            agent_result = await self.process_extracted_text(
                text=extracted_text,
                page_number=page_number,
                filename=filename
            )
            
            result["agent_result"] = agent_result
            result["success"] = agent_result.get("success", False)
            
        except Exception as e:
            result["error"] = str(e)
            print(f"âŒ Page {page_number}: Processing failed - {e}")
        
        result["processing_time"] = time.time() - start_time
        return result
    
    async def process_extracted_text(self, text: str, page_number: int, filename: str) -> Dict[str, Any]:
        """Process extracted text using agent pipeline"""
        
        start_time = time.time()
        result = {
            "success": False,
            "processing_time": 0,
            "extracted_data": {},
            "classification": {},
            "quality_assessment": {},
            "error": None
        }
        
        try:
            print(f"ðŸ¤– Processing text with agents...")
            
            # Step 1: Clean and improve OCR text
            cleaned_text = await self._clean_ocr_text(text)
            
            # Step 2: Extract entities
            extracted_data = await self._extract_entities(cleaned_text)
            result["extracted_data"] = extracted_data

            # Step 3: Classify document
            classification = await self._classify_document(cleaned_text)
            result["classification"] = classification
            
            result["success"] = True
            print(f"âœ… Agent processing completed")
            
        except Exception as e:
            result["error"] = str(e)
            print(f"âŒ Agent processing failed: {e}")
        
        result["processing_time"] = time.time() - start_time
        return result
    
    async def _clean_ocr_text(self, text: str) -> str:
        """Clean OCR text using OCR specialist agent"""
        try:
            prompt = f"Ù†Ø¸Ù ÙˆØ­Ø³Ù† Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ù† OCR:\n\n{text}"
            
            response = await self._call_groq_api(prompt, "OCR_Specialist")
            return response.get("content", text)
        except:
            return text  # Return original if cleaning fails
    
    async def _extract_entities(self, text: str) -> Dict[str, Any]:
        """Extract entities using extraction agent"""
        try:
            prompt = f"""Ø§Ø³ØªØ®Ø±Ø¬ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ:

Ø§Ù„Ù†Øµ: {text}

Ø§Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡ Ù…ÙÙŠØ¯ ÙˆØ§Ø³ØªØ®Ø±Ø¬Ù‡:
- Ø£ÙŠ Ø£Ø±Ù‚Ø§Ù… Ø£Ùˆ Ø£ÙƒÙˆØ§Ø¯ (117-11-2018, DL-123, 123456)
- Ø£ÙŠ ØªÙˆØ§Ø±ÙŠØ® (11/11/2018, 2018/11/11, 15-01-2024)
- Ø£ÙŠ Ø£Ø³Ù…Ø§Ø¡ (Ù…Ø­Ù…Ø¯, Ø³Ù„ÙŠÙ…, Ø£Ø­Ù…Ø¯, Ø¹Ù„ÙŠ)
- Ø£ÙŠ Ø£Ù…Ø§ÙƒÙ† (Ø·Ø±Ø§Ø¨Ù„Ø³, Ø¨ÙŠØ±ÙˆØª, Ø§Ù„ØªØ¨Ø§Ù†Ø©)
- Ø£ÙŠ Ù…Ø¤Ø³Ø³Ø§Øª (ÙˆØ²Ø§Ø±Ø©, Ø¥Ø¯Ø§Ø±Ø©, Ù…ÙƒØªØ¨)
- Ø£ÙŠ Ø£Ù†ÙˆØ§Ø¹ ÙˆØ«Ø§Ø¦Ù‚ (Ø¥ÙØ§Ø¯Ø©, Ø±Ø®ØµØ©, Ø´Ù‡Ø§Ø¯Ø©)

Ù‚ÙˆØ§Ø¹Ø¯ Ø¨Ø³ÙŠØ·Ø©:
1. Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ø£ÙŠ Ø±Ù‚Ù… Ø£Ùˆ ÙƒÙˆØ¯ â†’ Ø¶Ø¹Ù‡ ÙÙŠ Ø±Ù‚Ù…_Ø§Ù„Ù…Ø³ØªÙ†Ø¯
2. Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ø£ÙŠ ØªØ§Ø±ÙŠØ® â†’ Ø¶Ø¹Ù‡ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®
3. Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ø£ÙŠ Ø§Ø³Ù… Ø´Ø®Øµ â†’ Ø¶Ø¹Ù‡ ÙÙŠ Ø§Ù„Ø§Ø³Ù…_Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
4. Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ø£ÙŠ Ù…ÙƒØ§Ù† â†’ Ø¶Ø¹Ù‡ ÙÙŠ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
5. Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ø£ÙŠ Ù…Ø¤Ø³Ø³Ø© â†’ Ø¶Ø¹Ù‡ ÙÙŠ Ø§Ù„Ø¬Ù‡Ø©_Ø§Ù„ØµØ§Ø¯Ø±Ø©
6. Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ù†ÙˆØ¹ ÙˆØ«ÙŠÙ‚Ø© â†’ Ø¶Ø¹Ù‡ ÙÙŠ Ù†ÙˆØ¹_Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©
7. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø´ÙŠØ¡ â†’ Ø§ÙƒØªØ¨ "ØºÙŠØ± Ù…ØªÙˆÙØ±"

Ø§Ø³ØªØ®Ø±Ø¬ Ø£ÙŠ Ø´ÙŠØ¡ ØªØ¬Ø¯Ù‡ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù† ØºÙŠØ± Ù…ÙƒØªÙ…Ù„.

Ø£Ø¬Ø¨ Ø¨Ù€ JSON ÙÙ‚Ø·:
{{
    "Ø±Ù‚Ù…_Ø§Ù„Ù…Ø³ØªÙ†Ø¯": "...",
    "Ø§Ù„ØªØ§Ø±ÙŠØ®": "...",
    "Ù†ÙˆØ¹_Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©": "...",
    "Ø§Ù„Ø¬Ù‡Ø©_Ø§Ù„ØµØ§Ø¯Ø±Ø©": "...",
    "Ø§Ù„Ø§Ø³Ù…_Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ": "...",
    "Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„": "...",
    "Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹": "..."
}}"""

            response = await self._call_groq_api(prompt, "Entity_Extractor")
            content = response.get("content", "{}")

            # Clean the response to extract JSON only
            content = content.strip()

            # Try multiple patterns to extract JSON
            json_content = None

            # Pattern 1: ```json ... ```
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end != -1:
                    json_content = content[start:end].strip()

            # Pattern 2: ``` ... ``` (generic code block)
            elif "```" in content:
                start = content.find("```") + 3
                end = content.find("```", start)
                if end != -1:
                    json_content = content[start:end].strip()

            # Pattern 3: Look for { ... } pattern
            elif "{" in content and "}" in content:
                start = content.find("{")
                end = content.rfind("}") + 1
                if start != -1 and end > start:
                    json_content = content[start:end].strip()

            # Pattern 4: Use the whole content if it looks like JSON
            elif content.startswith("{") and content.endswith("}"):
                json_content = content
            else:
                json_content = content

            content = json_content

            # Try to parse JSON
            try:
                extracted_data = json.loads(content)
                # Ensure all expected fields are present
                default_fields = {
                    "Ø±Ù‚Ù…_Ø§Ù„Ù…Ø³ØªÙ†Ø¯": "ØºÙŠØ± Ù…ØªÙˆÙØ±",
                    "Ø§Ù„ØªØ§Ø±ÙŠØ®": "ØºÙŠØ± Ù…ØªÙˆÙØ±",
                    "Ù†ÙˆØ¹_Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                    "Ø§Ù„Ø¬Ù‡Ø©_Ø§Ù„ØµØ§Ø¯Ø±Ø©": "ØºÙŠØ± Ù…ØªÙˆÙØ±",
                    "Ø§Ù„Ø§Ø³Ù…_Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ": "ØºÙŠØ± Ù…ØªÙˆÙØ±",
                    "Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„": "ØºÙŠØ± Ù…ØªÙˆÙØ±",
                    "Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
                }

                # Merge with defaults
                for key, default_value in default_fields.items():
                    if key not in extracted_data:
                        extracted_data[key] = default_value

                # Add review step to filter duplicates and nonsensical data
                reviewed_data = await self._review_extracted_data(extracted_data)
                return reviewed_data
            except Exception as json_error:
                print(f"JSON parsing error: {json_error}")
                print(f"Raw content: {content}")
                # If JSON parsing fails, return structured fallback
                return {
                    "Ø±Ù‚Ù…_Ø§Ù„Ù…Ø³ØªÙ†Ø¯": "ØºÙŠØ± Ù…ØªÙˆÙØ±",
                    "Ø§Ù„ØªØ§Ø±ÙŠØ®": "ØºÙŠØ± Ù…ØªÙˆÙØ±",
                    "Ù†ÙˆØ¹_Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                    "Ø§Ù„Ø¬Ù‡Ø©_Ø§Ù„ØµØ§Ø¯Ø±Ø©": "ØºÙŠØ± Ù…ØªÙˆÙØ±",
                    "Ø§Ù„Ø§Ø³Ù…_Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ": "ØºÙŠØ± Ù…ØªÙˆÙØ±",
                    "Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„": "ØºÙŠØ± Ù…ØªÙˆÙØ±",
                    "Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
                }
        except Exception as e:
            print(f"Entity extraction error: {e}")
            return {"error": str(e)}

    async def _review_extracted_data(self, extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """Review and clean extracted data using review agent"""
        try:
            prompt = f"""Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© ÙˆØ­Ø³Ù‘Ù†Ù‡Ø§:

Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {json.dumps(extracted_data, ensure_ascii=False, indent=2)}

Ù‚ÙˆØ§Ø¹Ø¯ Ø¨Ø³ÙŠØ·Ø©:
1. Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª ØªØ³Ù…ÙŠØ© Ù…Ø«Ù„ "Ø§Ù„Ø§Ø³Ù…:" ÙÙŠ Ø§Ù„Ù‚ÙŠÙ…Ø©ØŒ Ø£Ø²Ù„Ù‡Ø§ ÙˆØ§Ø­ØªÙØ¸ Ø¨Ù…Ø§ Ø¨Ø¹Ø¯Ù‡Ø§
2. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù‚ÙŠÙ…Ø© ÙØ§Ø±ØºØ© Ø£Ùˆ Ù…Ø¬Ø±Ø¯ ØªØ³Ù…ÙŠØ©ØŒ Ø¶Ø¹ "ØºÙŠØ± Ù…ØªÙˆÙØ±"
3. Ø§Ø­ØªÙØ¸ Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„ØªÙˆØ§Ø±ÙŠØ® ÙˆØ§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙƒÙ…Ø§ Ù‡ÙŠ
4. Ù„Ø§ ØªØºÙŠÙ‘Ø± Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙÙŠØ¯

Ù…Ø«Ø§Ù„:
Ù‚Ø¨Ù„: {{"Ø§Ù„Ø§Ø³Ù…_Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ": "Ø§Ù„Ø§Ø³Ù…: Ù…Ø­Ù…Ø¯ Ø³Ù„ÙŠÙ…"}}
Ø¨Ø¹Ø¯: {{"Ø§Ù„Ø§Ø³Ù…_Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ": "Ù…Ø­Ù…Ø¯ Ø³Ù„ÙŠÙ…"}}

Ø£Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø© Ø¨Ù€ JSON ÙÙ‚Ø·:"""

            response = await self._call_groq_api(prompt, "Data_Reviewer")
            content = response.get("content", "{}")

            # Clean and parse JSON
            content = content.strip()
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()
            elif "{" in content and "}" in content:
                start = content.find("{")
                end = content.rfind("}") + 1
                content = content[start:end].strip()

            try:
                reviewed_data = json.loads(content)
                return reviewed_data
            except:
                # If review fails, return original data
                return extracted_data

        except Exception as e:
            print(f"Review error: {e}")
            return extracted_data
    
    async def _classify_document(self, text: str) -> Dict[str, Any]:
        """Classify document using classifier agent"""
        try:
            prompt = f"ØµÙ†Ù Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©:\n\n{text}"
            
            response = await self._call_groq_api(prompt, "Document_Classifier")
            content = response.get("content", "{}")
            
            try:
                return json.loads(content)
            except:
                return {
                    "Ù†ÙˆØ¹_Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©": "Ø£Ø®Ø±Ù‰",
                    "Ù…Ø³ØªÙˆÙ‰_Ø§Ù„Ø«Ù‚Ø©": "Ù…Ù†Ø®ÙØ¶",
                    "Ø§Ù„Ø³Ø¨Ø¨": "ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„"
                }
        except Exception as e:
            return {"error": str(e)}
    
    async def _assess_quality(self, extracted_data: Dict, classification: Dict) -> Dict[str, Any]:
        """Assess quality using QA agent"""
        try:
            assessment_input = {
                "extracted_data": extracted_data,
                "classification": classification
            }
            
            prompt = f"Ø±Ø§Ø¬Ø¹ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ù„ÙŠ:\n\n{json.dumps(assessment_input, ensure_ascii=False, indent=2)}"
            
            response = await self._call_groq_api(prompt, "Quality_Assurance")
            content = response.get("content", "{}")
            
            try:
                return json.loads(content)
            except:
                return {
                    "ØªÙ‚ÙŠÙŠÙ…_Ø§Ù„Ø¬ÙˆØ¯Ø©": "5",
                    "Ù…Ø³ØªÙˆÙ‰_Ø§Ù„Ø«Ù‚Ø©": "Ù…ØªÙˆØ³Ø·",
                    "Ù…Ù„Ø§Ø­Ø¸Ø§Øª_Ø¥Ø¶Ø§ÙÙŠØ©": "ØªÙ… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ"
                }
        except Exception as e:
            return {"error": str(e)}
    
    async def _call_groq_api(self, prompt: str, agent_role: str) -> Dict[str, Any]:
        """Call Groq API directly"""
        try:
            url = "https://api.groq.com/openai/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.groq_api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "llama3-70b-8192",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2000,
                "temperature": 0.1
            }
            
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "content": result["choices"][0]["message"]["content"]
                }
            else:
                return {
                    "success": False,
                    "error": f"API Error {response.status_code}"
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    async def test_connection(self) -> Dict[str, Any]:
        """Test Groq API connection"""
        try:
            test_prompt = "Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ - Ù‚Ù„ Ù…Ø±Ø­Ø¨Ø§"
            response = await self._call_groq_api(test_prompt, "test")
            
            return {
                "status": "healthy" if response.get("success") else "unhealthy",
                "response": response
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }
